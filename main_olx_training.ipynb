{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "from scripts.dataset import Dataset\n",
    "from scripts.classification_model import Classification_model\n",
    "from scripts.classification_model import Hyper_params,Lr,Lr_reduce_on_plateau,Early_stop,Learning_params\n",
    "from scripts.model_evaluator import Model_evaluator\n",
    "from scripts import classifier_utils\n",
    "from time import gmtime, strftime\n",
    "from random import uniform\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_session(gpu_fraction=0.8):\n",
    "    'Allocate gpu_fraction its memory for computattion'\n",
    "\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "KTF.set_session(get_session())\n",
    "tf.GraphKeys.VARIABLES = tf.GraphKeys.GLOBAL_VARIABLES\n",
    "cur_time = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "\n",
    "# This code supports tensorflow backend only\n",
    "assert K.image_dim_ordering() == 'tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "train_file = 'train_dataset.txt'\n",
    "test_dir = 'test'\n",
    "test_file = 'test_dataset.txt'\n",
    "CLASS_NAMES = ['1521','1703','1707','1729','1751','1755','1837','2037','2525','2526']\n",
    "PRETRAINED_MODEL = 'VGG19' #TODO: check using Resnet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1521', '1703', '1707', '1729', '1751', '1755', '1837', '2037', '2525', '2526']\n",
      "resizing images\n",
      "data/full/1521\n",
      "pre-processing data/full/1521 dir\n",
      "data/full/1703\n",
      "pre-processing data/full/1703 dir\n",
      "data/full/1707\n",
      "pre-processing data/full/1707 dir\n",
      "data/full/1729\n",
      "pre-processing data/full/1729 dir\n",
      "data/full/1751\n",
      "pre-processing data/full/1751 dir\n",
      "data/full/1755\n",
      "pre-processing data/full/1755 dir\n",
      "data/full/1837\n",
      "pre-processing data/full/1837 dir\n",
      "data/full/2037\n",
      "pre-processing data/full/2037 dir\n",
      "data/full/2525\n",
      "pre-processing data/full/2525 dir\n",
      "data/full/2526\n",
      "pre-processing data/full/2526 dir\n",
      "data/full/train/1521 created\n",
      "data/full/train/1703 created\n",
      "data/full/train/1707 created\n",
      "data/full/train/1729 created\n",
      "data/full/train/1751 created\n",
      "data/full/train/1755 created\n",
      "data/full/train/1837 created\n",
      "data/full/train/2037 created\n",
      "data/full/train/2525 created\n",
      "data/full/train/2526 created\n",
      "data/full/val/1521 created\n",
      "data/full/val/1703 created\n",
      "data/full/val/1707 created\n",
      "data/full/val/1729 created\n",
      "data/full/val/1751 created\n",
      "data/full/val/1755 created\n",
      "data/full/val/1837 created\n",
      "data/full/val/2037 created\n",
      "data/full/val/2525 created\n",
      "data/full/val/2526 created\n"
     ]
    }
   ],
   "source": [
    "# # # May run this this one time only\n",
    "\n",
    "## Copy images to data dir where each class images are in a specific folder \n",
    "classifier_utils.copy_images_to_class_dirs(train_file, data_dir, CLASS_NAMES)\n",
    "\n",
    "# # Split data to train val\n",
    "classifier_utils.resize_and_train_val_split(data_dir, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset\n",
      "data/train\n",
      "10 Classes detected:\n",
      "['1521', '1703', '1707', '1729', '1751', '1755', '1837', '2037', '2525', '2526']\n",
      "Loading 8793 unique images from data/train/ directory\n",
      "Found 8793 images belonging to 10 classes.\n",
      "loaded and augmented all images\n",
      "train shape: (17586, 224, 224, 3)\n",
      "Creating val dataset\n",
      "data/val\n",
      "10 Classes detected:\n",
      "['1521', '1703', '1707', '1729', '1751', '1755', '1837', '2037', '2525', '2526']\n",
      "Loading 1506 unique images from data/val/ directory\n",
      "Found 1506 images belonging to 10 classes.\n",
      "loaded and augmented all images\n",
      "train shape: (1506, 224, 224, 3)\n",
      "loading vgg 19 network\n"
     ]
    }
   ],
   "source": [
    "# Create experiment dirs (weights, summary)\n",
    "[experiments_dir, model_dir, weights_dir, summary_dir] = classifier_utils.create_experiment_directories()\n",
    "\n",
    "print 'Creating train dataset'\n",
    "train_dataset = Dataset(data_dir,'train', PRETRAINED_MODEL,\n",
    "                        class_names=CLASS_NAMES,\n",
    "                        augment_factor=2.0, fliplr=True,\n",
    "                        shift_range=0.2, zoom_range=0.2)\n",
    "print 'Creating val dataset'\n",
    "val_dataset = Dataset(data_dir,'val', PRETRAINED_MODEL,\n",
    "                        class_names=CLASS_NAMES)\n",
    "\n",
    "# Init params for first time - initial guess\n",
    "#Hyperparams\n",
    "hyper_params = Hyper_params(lr=Lr(init=0.0001,decay=0.0), optimization_method='Adam',\n",
    "                           momentum=0.9, weight_decay=0.0002, dropout_rate=0.35,\n",
    "                           freeze_layer_id=21, architecture_head_type='no_layers',\n",
    "                           is_use_batch_normalization=True, nb_filters_conv_1_1_first=128,\n",
    "                           nb_filters_conv_1_1_second=32, fc_layer_size_1=128, fc_layer_size_2=64)\n",
    "#Learning params\n",
    "learning_params = Learning_params(nb_train_samples=train_dataset.nb_samples,\n",
    "                                 nb_validation_samples=val_dataset.nb_samples,\n",
    "                                 batch_size=32, nb_max_epoch=150, \n",
    "                                 nb_samples_per_epoch=train_dataset.nb_samples ,\n",
    "                                 nb_validation_samples_per_epoch=val_dataset.nb_samples,\n",
    "                                 ckpt_period=1, early_stop=Early_stop(is_use=True, min_delta=0.005,patience=15),\n",
    "                                 lr_reduce_on_plateau=Lr_reduce_on_plateau(is_use=True, factor=0.5, min_lr=1e-08, patience=3))\n",
    "# Init classification model\n",
    "clf_model = Classification_model(train_dataset.nb_classes,\n",
    "                 weights_dir, summary_dir,\n",
    "                 'VGG19',False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # Hyper-Parameter search - don't run if not tuning, takes a lot of time !!\n",
    "\n",
    "# TODO: add iteration over network, augmentation on luminance, iteration over different augmentation factors\n",
    "\n",
    "\n",
    "# Iterate over hyperparameter space - currently only vgg19\n",
    "# hyperparams to iterate: head_type,# nb_filters_conv_1_1_first, nb_filters_conv_1_1_second, fc_layer_size_1,\n",
    "# fc_layer_size_2, freeze_layer, learning rate, weight decay,dropout_rate, batch size, \n",
    "# Init hyperparams and learning params with relevant values\n",
    "# run model - low number of epochs (1)\n",
    "# calc scores\n",
    "# print changed value only, and scores\n",
    "\n",
    "\n",
    "## Wasn't feasible on my machine to run over all hyper parameters, so by try and error I chose some of them and \n",
    "## and tuned the rest\n",
    "# head_type_opts = ['no_layers', 'one_fc_layer']\n",
    "# nb_filters_conv_1_1_first_opts = [16,32,64,128,256]\n",
    "# nb_filters_conv_1_1_second_opts = [8,16,32,64,128]\n",
    "# fc_layer_size_1_opts = [32,64,128,256,512,1024]\n",
    "# fc_layer_size_2_opts = [16,32,64,128,256,512]\n",
    "freeze_layer_id_opts = [20,21]#for resnet need to edit to last layers\n",
    "batch_size_opts = [16,32]\n",
    "\n",
    "max_clf_acc = 0.0\n",
    "max_clf_hyper_params = Hyper_params()\n",
    "max_clf_learning_params = Learning_params()\n",
    "\n",
    "learning_params.nb_max_epoch = 1 # restrict num epochs per test\n",
    "is_save_ckpts = False\n",
    "\n",
    "num_options = 2*2*3*3*3\n",
    "count = 0\n",
    "for freeze_layer_id in freeze_layer_id_opts:\n",
    "    hyper_params.freeze_layer_id = freeze_layer_id\n",
    "    for batch_size in batch_size_opts:        \n",
    "        for i in range(0,3):\n",
    "            for j in range(0,3):\n",
    "                for k in range(0,3):                     \n",
    "                    learning_rate = 10**uniform(-6,-1)\n",
    "                    hyper_params.lr.init = learning_rate\n",
    "                    weight_decay = 10**uniform(-6,0)\n",
    "                    hyper_params.weight_decay = weight_decay\n",
    "                    dropout_rate = uniform(0,1)\n",
    "                    hyper_params.dropout_rate = dropout_rate\n",
    "                   \n",
    "                    print '\\n\\n'    \n",
    "                    learning_params.batch_size = batch_size\n",
    "                    print 'freeze_layer_id: ' + str(freeze_layer_id)\n",
    "                    print 'learning_rate: ' + str(learning_rate)\n",
    "                    print 'weight_decay: ' + str(weight_decay)\n",
    "                    print 'dropout_rate: ' + str(dropout_rate)\n",
    "                    print 'batch_size: ' + str(batch_size) +'\\n'\n",
    "                    count = count + 1\n",
    "                    print 'processing option number ' + str(count) + ' out of ' + str(num_options)                                                            \n",
    "                    clf_model.build_model(input_shape=train_dataset.images[0,:,:,:].shape,\n",
    "                                          hyper_params=hyper_params)      \n",
    "                    clf_model.compile_model(learning_params, is_save_ckpts)\n",
    "                    clf_model.train(train_data=train_dataset.images,train_labels=train_dataset.labels,\n",
    "                            validation_data=val_dataset.images, validation_labels=val_dataset.labels)\n",
    "                    clf_model_evaluator = Model_evaluator(clf_model, train_dataset.class_names)\n",
    "                    clf_score = clf_model_evaluator.get_classifier_score(dataset=val_dataset)\n",
    "                    if clf_score[1] > max_clf_acc:\n",
    "                        print 'C + str(clf_score[1])\n",
    "                        max_clf_acc = clf_score[1]\n",
    "                        max_clf_hyper_params = hyper_params\n",
    "                        max_clf_learning_params = learning_params\n",
    "                    clf_model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep max clf_score set and params\n",
    "# Train model again - larger number of epochs\n",
    "\n",
    "# Load best parameter set\n",
    "max_clf_learning_params = learning_params\n",
    "max_clf_hyper_params = hyper_params\n",
    "max_clf_hyper_params.freeze_layer_id = 21\n",
    "max_clf_hyper_params.lr.init = 0.00102382988483\n",
    "max_clf_hyper_params.weight_decay = 0.000173410239311\n",
    "max_clf_hyper_params.dropout_rate = 0.491165771543\n",
    "max_clf_learning_params.batch_size = 32\n",
    "\n",
    "# Train over many epochs with early stop callback\n",
    "learning_params.nb_max_epoch = 100 # full train on best clf found\n",
    "is_save_ckpts = True\n",
    "\n",
    "is_use_classes_weights = False\n",
    "classes_weights = None\n",
    "if is_use_classes_weights:\n",
    "    classes_weights = train_dataset.classes_weights\n",
    "\n",
    "clf_model.build_model(input_shape=train_dataset.images[0,:,:,:].shape,\n",
    "                      hyper_params=max_clf_hyper_params)      \n",
    "clf_model.compile_model(max_clf_learning_params, is_save_ckpts)\n",
    "clf_model.train(train_dataset.images,train_dataset.labels,\n",
    "        val_dataset.images, val_dataset.labels, classes_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "clf_model_evaluator = Model_evaluator(clf_model, train_dataset.class_names)\n",
    "clf_score = clf_model_evaluator.get_classifier_score(dataset=val_dataset)\n",
    "clf_model_evaluator = Model_evaluator(clf_model, train_dataset.class_names)\n",
    "clf_model_evaluator.evaluate(dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save  best model weights to best model directory\n",
    "## All check points are in experiment directory, possible to load intermediate checkpoints if overfit occured\n",
    "if not os.path.exists('best_model'):\n",
    "        os.makedirs('best_model')\n",
    "clf_model.model.save_weights('best_model/best_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # # Production - Train over full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating production_train dataset\n",
      "data/full\n",
      "10 Classes detected:\n",
      "['1521', '1703', '1707', '1729', '1751', '1755', '1837', '2037', '2525', '2526']\n",
      "Loading 10299 unique images from data/full/ directory\n",
      "Found 10299 images belonging to 10 classes.\n",
      "loaded and augmented all images\n",
      "train shape: (20598, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = 'data'\n",
    "data_dir = os.path.join(data_dir,'full')\n",
    "is_production = True\n",
    "is_save_ckpts = False\n",
    "\n",
    "print 'creating production_train dataset'\n",
    "prod_train_dataset = Dataset(data_dir,'train', PRETRAINED_MODEL,\n",
    "                        class_names=CLASS_NAMES,\n",
    "                        is_production = True,\n",
    "                        augment_factor=2.0, fliplr=True,\n",
    "                        shift_range=0.2, zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create and train model, start training or load from scratch from last checkpoint\n",
    "\n",
    "# Had an unknown bug while loading and saving models, can't tell why this happened.\n",
    "# This is why I init a new model and loaded weights instead of loading using json files\n",
    "\n",
    "best_hyper_params = Hyper_params(lr=Lr(init=0.00102382988483,decay=0.0), optimization_method='Adam',\n",
    "                           momentum=0.9, weight_decay=0.000173410239311, dropout_rate=0.491165771543,\n",
    "                           freeze_layer_id=21, architecture_head_type='no_layers',\n",
    "                           is_use_batch_normalization=True, nb_filters_conv_1_1_first=128,\n",
    "                           nb_filters_conv_1_1_second=32, fc_layer_size_1=128, fc_layer_size_2=64)\n",
    "best_learning_params = Learning_params(nb_train_samples=prod_train_dataset.nb_samples,\n",
    "                                 batch_size=32, nb_max_epoch=150, \n",
    "                                 nb_samples_per_epoch=prod_train_dataset.nb_samples ,\n",
    "                                 ckpt_period=1, early_stop=Early_stop(is_use=True, min_delta=0.005,patience=15),\n",
    "                                 lr_reduce_on_plateau=Lr_reduce_on_plateau(is_use=True, factor=0.5, min_lr=1e-08, patience=3))\n",
    "best_weights_dir = 'best_model'\n",
    "best_summary_dir = best_weights_dir\n",
    "best_model = Classification_model(10, best_weights_dir, best_summary_dir, PRETRAINED_MODEL,False)\n",
    "best_model.build_model(input_shape=prod_train_dataset.images[0,:,:,:].shape,\n",
    "                       hyper_params=best_hyper_params)\n",
    "\n",
    "best_model.model.load_weights(os.path.join(best_weights_dir,'best_model/best_weights.h5'))\n",
    "best_model.compile_model(best_learning_params, is_save_ckpts, is_production)\n",
    "best_model.train(prod_train_dataset.images, prod_train_dataset.labels, is_production=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save production model \n",
    "best_model.model.save_weights('best_model/production_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate predictions on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying images to test dir\n",
      "pre-processing test/images dir\n",
      "Found 4674 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Copy images to test dir \n",
    "\n",
    "test_images = classifier_utils.load_test_images(test_file, test_dir,PRETRAINED_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict images and save\n",
    "predictions = best_model.model.predict(test_images)\n",
    "out_classes = np.argmax(predictions, axis=1)\n",
    "classifier_utils.generate_output_file(test_file, 'results_test.txt', out_classes, CLASS_NAMES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
