{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thoghts\n",
    "# Add more pretrained networks\n",
    "# Use additional data such as define an item as number od images:\n",
    "# - run each network over every image and decide using weighting voting \n",
    "#   using softmax probability output) to decide  \n",
    "# - Give a weight to each image in an item using prior knowledge - for example if the first image is usually\n",
    "#   the most important etc.\n",
    "# - Novel architecture - run each image of an item over some or all conv/relu/pooling layers,\n",
    "#   then concat output and add additional fully connected layers and get one category output per item\n",
    "#   * This will probably involve some 1X1 conv to decrease dimensonality as the first fully connected\n",
    "#     will be extremly large\n",
    "# - Use bigger size for input images or original size with average pooling (fc constraints input size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assumptions:\n",
    "# Multi-class problem (and not Multi label)\n",
    "# I used every image independently and didn't added the item correlation. The easiest and fastest way\n",
    "# to add this prior knowledge is by voting all outputs for each unique item \n",
    "# though there are 11000 training exam, I succeeded fetching only 10000, didn't bother me that much as\n",
    "# it was equally seperated between the classes\n",
    "\n",
    "# - I've neglected ad index as (assumption above)\n",
    "# - As I work with Keras it was easier for me for me to copy all images and set them in unique folders\n",
    "#   for each class  For viszualization purposes\n",
    "\n",
    "# Phases:\n",
    "# - Set images in dirs by categories\n",
    "# - Examine - \n",
    "# * High imbalance dataset!!! options - augment a lot most of the data, use weighted loss\n",
    "# * Trained again with weighted loss\n",
    "# * Specifically, some classes are strongly related to others. It'll surely increase accuracy by merging\n",
    "#   categories and first classify shoes,jewlery,cosmetics,Accessories,watches and clothing.\n",
    "#   Then we can go deeper to each sub category - one classifier for shoes and wedding shoes,\n",
    "#   and one classifier for clothing,maternity clothong, wedding suits and wedding dresses.  \n",
    "#   This will help in the imbalance problem in an order of magnitude, for other orders it will be best\n",
    "#   to augment more samples from low classes\n",
    "# * better to first merge these classes and then classiffy solely between shoes and wedding shoes \n",
    "# - Resize to 224*224 (vgg constraint) and examine if sufficient (it will be seen by the results ),\n",
    "#   Keep aspect ratio by padding with zeros in image borders\n",
    "#   preprocess as in vgg(subtract mean)\n",
    "# - Train Val split - 8732 train and 1500 validation\n",
    "# - augmentation\n",
    "#  * did mirroring, shift and zoom. didn't did rotation as I assume this will ruin the inner structure\n",
    "#   of the data. It's possible to also use luminance changes (convert images to lab space) . augment 2X images\n",
    "# - Examine\n",
    "# - Decide a method to work - because no prior knowledge and time to extract features - obviously cnn's\n",
    "#  * Imagenet for the rescue, I used VGG19 due to it's simplicity of structure and proven results.\n",
    "#   With more time I would also try Resnet50 \n",
    "# - Only finetune (not enought data and natural images)\n",
    "# - Start drilling trainable \n",
    "# - Simple fine tune for last layer didn't yield\n",
    "# - Drill in in trainable layers to see where overfit is inevitable, I got that training any convolution\n",
    "#   layer provides a big overfit, So I trained only my addition for the network\n",
    "# - Random search over parameter space (log space) for one epoch\n",
    "#   Chose best hyper-parameter and trained - reached 83% on validation, high imbalance\n",
    "#   By examining conf matrix one can see problem of imbalance. Learning curves was hard to  \n",
    "#   investigate due the fact that networked converged pretty quickly, with more data it would be an option.\n",
    "#   Data learning curve could have been nice but there wasn't an option to add more data so I didn't compute\n",
    "#   Used weighted loss - though improving as expected the precision and recall (write what they are)\n",
    "# - Choose best hyper-parameter set and fully train over full training data (10500)\n",
    "#   It didn't improve the accuracy so I preffered to stick with the balanced loss model.\n",
    "# - Train over all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "from scripts.dataset import Dataset\n",
    "from scripts.classification_model import Classification_model\n",
    "from scripts.classification_model import Hyper_params,Lr,Lr_reduce_on_plateau,Early_stop,Learning_params\n",
    "from scripts.model_evaluator import Model_evaluator\n",
    "from scripts import classifier_utils\n",
    "from time import gmtime, strftime\n",
    "from random import uniform\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_session(gpu_fraction=0.8):\n",
    "    'Allocate gpu_fraction its memory for computattion'\n",
    "\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "KTF.set_session(get_session())\n",
    "tf.GraphKeys.VARIABLES = tf.GraphKeys.GLOBAL_VARIABLES\n",
    "cur_time = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "\n",
    "# This code supports tensorflow backend only\n",
    "assert K.image_dim_ordering() == 'tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "train_file = 'train_dataset.txt'\n",
    "test_dir = 'test'\n",
    "test_file = 'test_dataset.txt'\n",
    "CLASS_NAMES = ['1521','1703','1707','1729','1751','1755','1837','2037','2525','2526']\n",
    "PRETRAINED_MODEL = 'VGG19' #TODO: check using Resnet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1521', '1703', '1707', '1729', '1751', '1755', '1837', '2037', '2525', '2526']\n",
      "resizing images\n",
      "data/1521\n",
      "pre-processingdata/1521 dir\n",
      "data/1703\n",
      "pre-processingdata/1703 dir\n",
      "data/1707\n",
      "pre-processingdata/1707 dir\n",
      "data/1729\n",
      "pre-processingdata/1729 dir\n",
      "data/1751\n",
      "pre-processingdata/1751 dir\n",
      "data/1755\n",
      "pre-processingdata/1755 dir\n",
      "data/1837\n",
      "pre-processingdata/1837 dir\n",
      "data/2037\n",
      "pre-processingdata/2037 dir\n",
      "data/2525\n",
      "pre-processingdata/2525 dir\n",
      "data/2526\n",
      "pre-processingdata/2526 dir\n",
      "data/train/1521 created\n",
      "data/train/1703 created\n",
      "data/train/1707 created\n",
      "data/train/1729 created\n",
      "data/train/1751 created\n",
      "data/train/1755 created\n",
      "data/train/1837 created\n",
      "data/train/2037 created\n",
      "data/train/2525 created\n",
      "data/train/2526 created\n",
      "data/val/1521 created\n",
      "data/val/1703 created\n",
      "data/val/1707 created\n",
      "data/val/1729 created\n",
      "data/val/1751 created\n",
      "data/val/1755 created\n",
      "data/val/1837 created\n",
      "data/val/2037 created\n",
      "data/val/2525 created\n",
      "data/val/2526 created\n"
     ]
    }
   ],
   "source": [
    "# # # May run this this one time only\n",
    "\n",
    "## Copy images to data dir where each class images are in a specific folder \n",
    "classifier_utils.copy_images_to_class_dirs(train_file, data_dir, CLASS_NAMES)\n",
    "\n",
    "# # Split data to train val\n",
    "classifier_utils.resize_and_train_val_split(data_dir, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataset\n",
      "data/train\n",
      "10 Classes detected:\n",
      "['1521', '1703', '1707', '1729', '1751', '1755', '1837', '2037', '2525', '2526']\n",
      "Loading 8698 unique images from data/train/ directory\n",
      "Found 8698 images belonging to 10 classes.\n",
      "loaded and augmented all images\n",
      "train shape: (17396, 224, 224, 3)\n",
      "Creating val dataset\n",
      "data/val\n",
      "10 Classes detected:\n",
      "['1521', '1703', '1707', '1729', '1751', '1755', '1837', '2037', '2525', '2526']\n",
      "Loading 1601 unique images from data/val/ directory\n",
      "Found 1601 images belonging to 10 classes.\n",
      "loaded and augmented all images\n",
      "train shape: (1601, 224, 224, 3)\n",
      "loading vgg 19 network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, None, None, 64 1792        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, None, None, 64 36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, None, None, 64 0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, None, None, 12 73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, None, None, 12 147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, None, None, 12 0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, None, None, 25 295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, None, None, 25 590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, None, None, 25 590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv4 (Convolution2D)     (None, None, None, 25 590080      block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, None, None, 25 0           block3_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, None, None, 51 1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, None, None, 51 2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, None, None, 51 2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv4 (Convolution2D)     (None, None, None, 51 2359808     block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, None, None, 51 0           block4_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, None, None, 51 2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, None, None, 51 2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, None, None, 51 2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv4 (Convolution2D)     (None, None, None, 51 2359808     block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, None, None, 51 0           block5_conv4[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create experiment dirs (weights, summary)\n",
    "[experiments_dir, model_dir, weights_dir, summary_dir] = classifier_utils.create_experiment_directories()\n",
    "\n",
    "print 'Creating train dataset'\n",
    "train_dataset = Dataset(data_dir,'train', PRETRAINED_MODEL,\n",
    "                        class_names=CLASS_NAMES,\n",
    "                        augment_factor=2.0, fliplr=True,\n",
    "                        shift_range=0.2, zoom_range=0.2)\n",
    "print 'Creating val dataset'\n",
    "val_dataset = Dataset(data_dir,'val', PRETRAINED_MODEL,\n",
    "                        class_names=CLASS_NAMES)\n",
    "\n",
    "# Init params for first time - initial guess\n",
    "#Hyperparams\n",
    "hyper_params = Hyper_params(lr=Lr(init=0.0001,decay=0.0), optimization_method='Adam',\n",
    "                           momentum=0.9, weight_decay=0.0002, dropout_rate=0.35,\n",
    "                           freeze_layer_id=21, architecture_head_type='no_layers',\n",
    "                           is_use_batch_normalization=True, nb_filters_conv_1_1_first=128,\n",
    "                           nb_filters_conv_1_1_second=32, fc_layer_size_1=128, fc_layer_size_2=64)\n",
    "#Learning params\n",
    "learning_params = Learning_params(nb_train_samples=train_dataset.nb_samples,\n",
    "                                 nb_validation_samples=val_dataset.nb_samples,\n",
    "                                 batch_size=32, nb_max_epoch=150, \n",
    "                                 nb_samples_per_epoch=train_dataset.nb_samples ,\n",
    "                                 nb_validation_samples_per_epoch=val_dataset.nb_samples,\n",
    "                                 ckpt_period=1, early_stop=Early_stop(is_use=True, min_delta=0.005,patience=15),\n",
    "                                 lr_reduce_on_plateau=Lr_reduce_on_plateau(is_use=True, factor=0.5, min_lr=1e-08, patience=3))\n",
    "# Init classification model\n",
    "clf_model = Classification_model(train_dataset.nb_classes,\n",
    "                 weights_dir, summary_dir,\n",
    "                 'VGG19',False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # Hyper-Parameter search - don't run if not tuning, takes a lot of time !!\n",
    "\n",
    "# TODO: add iteration over network, augmentation on luminance, iteration over different augmentation factors\n",
    "\n",
    "\n",
    "# Iterate over hyperparameter space - currently only vgg19\n",
    "# hyperparams to iterate: head_type,# nb_filters_conv_1_1_first, nb_filters_conv_1_1_second, fc_layer_size_1,\n",
    "# fc_layer_size_2, freeze_layer, learning rate, weight decay,dropout_rate, batch size, \n",
    "# Init hyperparams and learning params with relevant values\n",
    "# run model - low number of epochs (1)\n",
    "# calc scores\n",
    "# print changed value only, and scores\n",
    "\n",
    "\n",
    "## Wasn't feasible on my machine to run over all hyper parameters, so by try and error I chose some of them and \n",
    "## and tuned the rest\n",
    "# head_type_opts = ['no_layers', 'one_fc_layer']\n",
    "# nb_filters_conv_1_1_first_opts = [16,32,64,128,256]\n",
    "# nb_filters_conv_1_1_second_opts = [8,16,32,64,128]\n",
    "# fc_layer_size_1_opts = [32,64,128,256,512,1024]\n",
    "# fc_layer_size_2_opts = [16,32,64,128,256,512]\n",
    "freeze_layer_id_opts = [20,21]#for resnet need to edit to last layers\n",
    "batch_size_opts = [16,32]\n",
    "\n",
    "max_clf_acc = 0.0\n",
    "max_clf_hyper_params = Hyper_params()\n",
    "max_clf_learning_params = Learning_params()\n",
    "\n",
    "learning_params.nb_max_epoch = 1 # restrict num epochs per test\n",
    "is_save_ckpts = False\n",
    "\n",
    "num_options = 2*2*3*3*3\n",
    "count = 0\n",
    "for freeze_layer_id in freeze_layer_id_opts:\n",
    "    hyper_params.freeze_layer_id = freeze_layer_id\n",
    "    for batch_size in batch_size_opts:        \n",
    "        for i in range(0,3):\n",
    "            for j in range(0,3):\n",
    "                for k in range(0,3):                     \n",
    "                    learning_rate = 10**uniform(-6,-1)\n",
    "                    hyper_params.lr.init = learning_rate\n",
    "                    weight_decay = 10**uniform(-6,0)\n",
    "                    hyper_params.weight_decay = weight_decay\n",
    "                    dropout_rate = uniform(0,1)\n",
    "                    hyper_params.dropout_rate = dropout_rate\n",
    "                   \n",
    "                    print '\\n\\n'    \n",
    "                    learning_params.batch_size = batch_size\n",
    "                    print 'freeze_layer_id: ' + str(freeze_layer_id)\n",
    "                    print 'learning_rate: ' + str(learning_rate)\n",
    "                    print 'weight_decay: ' + str(weight_decay)\n",
    "                    print 'dropout_rate: ' + str(dropout_rate)\n",
    "                    print 'batch_size: ' + str(batch_size) +'\\n'\n",
    "                    count = count + 1\n",
    "                    print 'processing option number ' + str(count) + ' out of ' + str(num_options)                                                            \n",
    "                    clf_model.build_model(input_shape=train_dataset.images[0,:,:,:].shape,\n",
    "                                          hyper_params=hyper_params)      \n",
    "                    clf_model.compile_model(learning_params, is_save_ckpts)\n",
    "                    clf_model.train(train_data=train_dataset.images,train_labels=train_dataset.labels,\n",
    "                            validation_data=val_dataset.images, validation_labels=val_dataset.labels)\n",
    "                    clf_model_evaluator = Model_evaluator(clf_model, train_dataset.class_names)\n",
    "                    clf_score = clf_model_evaluator.get_classifier_score(dataset=val_dataset)\n",
    "                    if clf_score[1] > max_clf_acc:\n",
    "                        print 'max clf acc found!! acc =  ' + str(clf_score[1])\n",
    "                        max_clf_acc = clf_score[1]\n",
    "                        max_clf_hyper_params = hyper_params\n",
    "                        max_clf_learning_params = learning_params\n",
    "                    clf_model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 image_input\n",
      "1 vgg19\n",
      "2 bottlekneck_1_1_conv_1\n",
      "3 batch_norm_1\n",
      "4 bottlekneck_1_1_conv_2\n",
      "5 batch_norm_2\n",
      "6 flatten\n",
      "7 dropout\n",
      "8 predictions\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "image_input (InputLayer)         (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "vgg19 (Model)                    multiple              20024384    image_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bottlekneck_1_1_conv_1 (Convolut (None, 7, 7, 128)     65664       vgg19[1][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_norm_1 (BatchNormalization (None, 7, 7, 128)     512         bottlekneck_1_1_conv_1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "bottlekneck_1_1_conv_2 (Convolut (None, 7, 7, 32)      4128        batch_norm_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_norm_2 (BatchNormalization (None, 7, 7, 32)      128         bottlekneck_1_1_conv_2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 1568)          0           batch_norm_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 1568)          0           flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 10)            15690       dropout[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 20,110,506\n",
      "Trainable params: 85,802\n",
      "Non-trainable params: 20,024,704\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "image_input (InputLayer)         (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "vgg19 (Model)                    multiple              20024384    image_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bottlekneck_1_1_conv_1 (Convolut (None, 7, 7, 128)     65664       vgg19[1][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_norm_1 (BatchNormalization (None, 7, 7, 128)     512         bottlekneck_1_1_conv_1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "bottlekneck_1_1_conv_2 (Convolut (None, 7, 7, 32)      4128        batch_norm_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_norm_2 (BatchNormalization (None, 7, 7, 32)      128         bottlekneck_1_1_conv_2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 1568)          0           batch_norm_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 1568)          0           flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 10)            15690       dropout[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 20,110,506\n",
      "Trainable params: 85,802\n",
      "Non-trainable params: 20,024,704\n",
      "____________________________________________________________________________________________________\n",
      "Train on 17396 samples, validate on 1601 samples\n",
      "Epoch 1/100\n",
      "17376/17396 [============================>.] - ETA: 0s - loss: 0.9644 - acc: 0.7232Epoch 00000: val_acc improved from inf to 0.81449, saving model to experiments/2017-08-27 15:52:48/weights/_weights-improvement-00-0.814-0.659_2017-08-27 15:54:39.hdf5\n",
      "17396/17396 [==============================] - 85s - loss: 0.9640 - acc: 0.7231 - val_loss: 0.6591 - val_acc: 0.8145\n",
      "Epoch 2/100\n",
      "10720/17396 [=================>............] - ETA: 31s - loss: 0.6020 - acc: 0.8233Training summary\n",
      "['acc', 'loss', 'val_acc', 'val_loss']\n",
      "[[0.72309726377990668], [0.96400430238666301], [0.81449094316052473], [0.65914514785554545]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntTIsJEmtDAtJUqv/DxgipAoKC10jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c71c01150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmaW5bAwM7Msh4WZmWU5LMzMLOv/A2cBZ4zQtrj0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c712af850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep max clf_score set and params\n",
    "# Train model again - larger number of epochs\n",
    "\n",
    "# Load best parameter set\n",
    "max_clf_learning_params = learning_params\n",
    "max_clf_hyper_params = hyper_params\n",
    "max_clf_hyper_params.freeze_layer_id = 21\n",
    "max_clf_hyper_params.lr.init = 0.00102382988483\n",
    "max_clf_hyper_params.weight_decay = 0.000173410239311\n",
    "max_clf_hyper_params.dropout_rate = 0.491165771543\n",
    "max_clf_learning_params.batch_size = 32\n",
    "\n",
    "# Train over many epochs with early stop callback\n",
    "learning_params.nb_max_epoch = 100 # full train on best clf found\n",
    "is_save_ckpts = True\n",
    "\n",
    "is_use_classes_weights = False\n",
    "classes_weights = None\n",
    "if is_use_classes_weights:\n",
    "    classes_weights = train_dataset.classes_weights\n",
    "\n",
    "clf_model.build_model(input_shape=train_dataset.images[0,:,:,:].shape,\n",
    "                      hyper_params=max_clf_hyper_params)      \n",
    "clf_model.compile_model(max_clf_learning_params, is_save_ckpts)\n",
    "clf_model.train(train_dataset.images,train_dataset.labels,\n",
    "        val_dataset.images, val_dataset.labels, classes_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1601 [============================>.] - ETA: 0sval set loss: 0.605322556225\n",
      "val set accuracy: 0.826358525921\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       1521       0.87      0.88      0.88       312\n",
      "       1703       0.77      0.74      0.75        97\n",
      "       1707       0.75      0.77      0.76        81\n",
      "       1729       0.64      0.71      0.67       187\n",
      "       1751       0.85      0.80      0.82       111\n",
      "       1755       0.87      0.92      0.90       712\n",
      "       1837       0.89      0.64      0.75        39\n",
      "       2037       0.00      0.00      0.00        29\n",
      "       2525       0.69      0.38      0.49        24\n",
      "       2526       0.00      0.00      0.00         9\n",
      "\n",
      "avg / total       0.81      0.83      0.81      1601\n",
      "\n",
      "[[276   2   4  17   2   7   0   0   4   0]\n",
      " [  0  72   2  10   5   8   0   0   0   0]\n",
      " [  4   3  62   4   2   6   0   0   0   0]\n",
      " [ 10   7   7 132   4  27   0   0   0   0]\n",
      " [  1   4   5   7  89   5   0   0   0   0]\n",
      " [ 15   1   3  31   2 658   2   0   0   0]\n",
      " [  1   0   0   2   1  10  25   0   0   0]\n",
      " [  1   2   0   0   0  26   0   0   0   0]\n",
      " [ 10   2   0   2   0   1   0   0   9   0]\n",
      " [  0   1   0   0   0   7   1   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/sklearn/metrics/metrics.py:1905: UserWarning: The sum of true positives and false positives are equal to zero for some labels. Precision is ill defined for those labels [7 9]. The precision and recall are equal to zero for some labels. fbeta_score is ill defined for those labels [7 9]. \n",
      "  average=None)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "clf_model_evaluator = Model_evaluator(clf_model, train_dataset.class_names)\n",
    "clf_score = clf_model_evaluator.get_classifier_score(dataset=val_dataset)\n",
    "clf_model_evaluator = Model_evaluator(clf_model, train_dataset.class_names)\n",
    "clf_model_evaluator.evaluate(dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save  best model weights to best model directory\n",
    "## All check points are in experiment directory, possible to load intermediate checkpoints if overfit occured\n",
    "if not os.path.exists('best_model'):\n",
    "        os.makedirs('best_model')\n",
    "clf_model.model.save_weights('best_model/best_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # # Production - Train over full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating production_train dataset\n",
      "data/full\n",
      "10 Classes detected:\n",
      "['1521', '1703', '1707', '1729', '1751', '1755', '1837', '2037', '2525', '2526']\n",
      "Loading 10299 unique images from data/full/ directory\n",
      "Found 10299 images belonging to 10 classes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bee77d3638f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mis_production\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0maugment_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfliplr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                         shift_range=0.2, zoom_range=0.2)\n\u001b[0m",
      "\u001b[0;32m/root/olx/scripts/dataset.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, dataset_name, pretrained_model, class_names, is_resize, target_size, augment_factor, fliplr, rotate_range, shift_range, zoom_range, luminance_range, is_production)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         self.load_and_augment_images(fliplr, rotate_range,\n\u001b[0;32m---> 72\u001b[0;31m                  shift_range, zoom_range, luminance_range)\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/olx/scripts/dataset.pyc\u001b[0m in \u001b[0;36mload_and_augment_images\u001b[0;34m(self, fliplr, rotate_range, shift_range, zoom_range, luminance_range)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mnum_images_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_images_gen\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m                            target_size=self.target_size)\n\u001b[1;32m    692\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_ordering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_ordering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mrandom_transform\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mtransform_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_matrix_offset_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         x = apply_transform(x, transform_matrix, img_channel_index,\n\u001b[0;32m--> 403\u001b[0;31m                             fill_mode=self.fill_mode, cval=self.cval)\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_shift_range\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_channel_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_shift_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_channel_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(x, transform_matrix, channel_index, fill_mode, cval)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mfinal_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     channel_images = [ndi.interpolation.affine_transform(x_channel, final_affine_matrix,\n\u001b[0;32m--> 109\u001b[0;31m                       final_offset, order=0, mode=fill_mode, cval=cval) for x_channel in x]\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/ndimage/interpolation.pyc\u001b[0m in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         _geometric_transform(filtered, None, None, matrix, offset,\n\u001b[0;32m--> 470\u001b[0;31m                              output, order, mode, cval, None, None)\n\u001b[0m\u001b[1;32m    471\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/ndimage/interpolation.pyc\u001b[0m in \u001b[0;36m_geometric_transform\u001b[0;34m(input, mapping, coordinates, matrix, offset, output, order, mode, cval, extra_arguments, extra_keywords)\u001b[0m\n\u001b[1;32m    130\u001b[0m     _nd_image.geometric_transform(\n\u001b[1;32m    131\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         order, mode, cval, extra_arguments, extra_keywords)\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = 'data'\n",
    "data_dir = os.path.join(data_dir,'full')\n",
    "is_production = True\n",
    "is_save_ckpts = False\n",
    "\n",
    "print 'creating production_train dataset'\n",
    "prod_train_dataset = Dataset(data_dir,'train', PRETRAINED_MODEL,\n",
    "                        class_names=CLASS_NAMES,\n",
    "                        is_production = True,\n",
    "                        augment_factor=2.0, fliplr=True,\n",
    "                        shift_range=0.2, zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vgg 19 network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, None, None, 64 1792        input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, None, None, 64 36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, None, None, 64 0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, None, None, 12 73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, None, None, 12 147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, None, None, 12 0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, None, None, 25 295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, None, None, 25 590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, None, None, 25 590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv4 (Convolution2D)     (None, None, None, 25 590080      block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, None, None, 25 0           block3_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, None, None, 51 1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, None, None, 51 2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, None, None, 51 2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv4 (Convolution2D)     (None, None, None, 51 2359808     block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, None, None, 51 0           block4_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, None, None, 51 2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, None, None, 51 2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, None, None, 51 2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv4 (Convolution2D)     (None, None, None, 51 2359808     block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, None, None, 51 0           block5_conv4[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "20598/20598 [==============================] - 91s - loss: 0.6525 - acc: 0.8079    \n",
      "Epoch 2/150\n",
      " 1728/20598 [=>............................] - ETA: 86s - loss: 0.4319 - acc: 0.8709"
     ]
    }
   ],
   "source": [
    "# Create and train model, start training or load from scratch from last checkpoint\n",
    "\n",
    "# Had an unknown bug while loading and saving models, can't tell why this happened.\n",
    "# This is why I init a new model and loaded weights instead of loading using json files\n",
    "\n",
    "best_hyper_params = Hyper_params(lr=Lr(init=0.00102382988483,decay=0.0), optimization_method='Adam',\n",
    "                           momentum=0.9, weight_decay=0.000173410239311, dropout_rate=0.491165771543,\n",
    "                           freeze_layer_id=21, architecture_head_type='no_layers',\n",
    "                           is_use_batch_normalization=True, nb_filters_conv_1_1_first=128,\n",
    "                           nb_filters_conv_1_1_second=32, fc_layer_size_1=128, fc_layer_size_2=64)\n",
    "best_learning_params = Learning_params(nb_train_samples=prod_train_dataset.nb_samples,\n",
    "                                 batch_size=32, nb_max_epoch=150, \n",
    "                                 nb_samples_per_epoch=prod_train_dataset.nb_samples ,\n",
    "                                 ckpt_period=1, early_stop=Early_stop(is_use=True, min_delta=0.005,patience=15),\n",
    "                                 lr_reduce_on_plateau=Lr_reduce_on_plateau(is_use=True, factor=0.5, min_lr=1e-08, patience=3))\n",
    "best_weights_dir = 'best_model'\n",
    "best_summary_dir = best_weights_dir\n",
    "best_model = Classification_model(10, best_weights_dir, best_summary_dir, PRETRAINED_MODEL,False)\n",
    "best_model.build_model(input_shape=prod_train_dataset.images[0,:,:,:].shape,\n",
    "                       hyper_params=best_hyper_params)\n",
    "\n",
    "best_model.model.load_weights(os.path.join(best_weights_dir,'best_weights.h5'))\n",
    "best_model.compile_model(best_learning_params, is_save_ckpts, is_production)\n",
    "best_model.train(prod_train_dataset.images, prod_train_dataset.labels, is_production=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save production model \n",
    "best_model.model.save_weights('best_model/production_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate predictions on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying images to test dir\n",
      "pre-processing test/images dir\n",
      "Found 4674 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Copy images to test dir \n",
    "\n",
    "test_images = classifier_utils.load_test_images(test_file, test_dir,PRETRAINED_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict images and save\n",
    "predictions = best_model.model.predict(test_images)\n",
    "out_classes = np.argmax(predictions, axis=1)\n",
    "classifier_utils.generate_output_file(test_file, 'results_test.txt', out_classes, CLASS_NAMES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
